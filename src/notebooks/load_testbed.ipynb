{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.autograd as autograd\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# best_model_path = '../../tr_models/classifierX3_best.pth'\n",
    "# last_model_path = '../../tr_models/classifierX3_last.pth'\n",
    "# plot_vals_path = '../../outputs/classifierX3_plotres.pkl'\n",
    "\n",
    "tr_labels = pd.read_csv('../../data/small_ntu_frames/train_labels', header=None)\n",
    "v_labels = pd.read_csv('../../data/small_ntu_frames/val_labels', header=None)\n",
    "\n",
    "with open('../../data/small_ntu_frames/train_data.pkl', 'rb') as f:\n",
    "    tr_dict = pickle.load(f)\n",
    "with open('../../data/small_ntu_frames/val_data.pkl', 'rb') as f:\n",
    "    v_dict = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_frames(arr_3d):\n",
    "    n_frames = arr_3d.shape[0]\n",
    "    temp_arr = np.zeros((n_frames, 48))\n",
    "    for i in range(n_frames):\n",
    "        arr_3d[i, :, :] = arr_3d[i, :, :] - arr_3d[i, 7, :]\n",
    "        norz = np.linalg.norm(arr_3d[1, 6, :] - arr_3d[i, 7, :])\n",
    "        arr_3d[i, :, :] /= 1.0*norz\n",
    "        temp_arr[i, :] = np.reshape(arr_3d[i, :, :], (1, -1))\n",
    "    return temp_arr\n",
    "\n",
    "def flatten_dict(d_dict):\n",
    "    for i in d_dict:\n",
    "        d_dict[i] = flatten_frames(d_dict[i])\n",
    "    return d_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_d = flatten_dict(tr_dict)\n",
    "v_d = flatten_dict(v_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_gen(d_dict, d_labels):\n",
    "    while(True):\n",
    "        d_id = random.sample(list(d_dict), 1)[0]\n",
    "        databatch = d_dict[d_id]\n",
    "        if 'train' in d_id:\n",
    "            idx = int(d_id.replace('train', ''))\n",
    "            label = d_labels.iloc[idx, 1]\n",
    "        elif 'val' in d_id:\n",
    "            idx = int(d_id.replace('val', ''))\n",
    "            label = d_labels.iloc[idx, 1]\n",
    "        yield databatch,label\n",
    "\n",
    "def val_data_gen(d_dict, d_labels):\n",
    "    counter = 0\n",
    "    n_s = len(d_dict)\n",
    "    while(counter<n_s):\n",
    "        d_id = list(d_dict)[counter]\n",
    "        databatch = d_dict[d_id]\n",
    "        if 'val' in d_id:\n",
    "            idx = int(d_id.replace('val', ''))\n",
    "            label = d_labels.iloc[idx, 1]\n",
    "        counter += 1\n",
    "        counter = counter % n_s\n",
    "        yield databatch,label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "Dtr = data_gen(tr_d, tr_labels)\n",
    "Dv = data_gen(v_d, v_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#action LSTM\n",
    "class LSTMClassifier(nn.Module):\n",
    "    def __init__(self, joints_dim, hidden_dim, label_size, batch_size, num_layers, kernel_size):\n",
    "        super(LSTMClassifier, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.batch_size = batch_size\n",
    "        self.num_layers = num_layers\n",
    "        joints_dim2d = joints_dim - 16\n",
    "        \n",
    "        self.lstm3 = nn.LSTM(joints_dim, hidden_dim, num_layers=self.num_layers)\n",
    "        self.conv1_3 = nn.Conv1d(1, 1, kernel_size, stride=1, padding=1)\n",
    "        \n",
    "        self.lstm2_1 = nn.LSTM(joints_dim2d, hidden_dim, num_layers=self.num_layers)\n",
    "        self.conv1_2_1 = nn.Conv1d(1, 1, kernel_size, stride=1, padding=1)\n",
    "        self.lstm2_2 = nn.LSTM(joints_dim2d, hidden_dim, num_layers=self.num_layers)\n",
    "        self.conv1_2_2 = nn.Conv1d(1, 1, kernel_size, stride=1, padding=1)\n",
    "        self.lstm2_3 = nn.LSTM(joints_dim2d, hidden_dim, num_layers=self.num_layers)\n",
    "        self.conv1_2_3 = nn.Conv1d(1, 1, kernel_size, stride=1, padding=1)\n",
    "        \n",
    "#         self.conv1_1 = nn.Conv1d(4, 2, kernel_size, stride=1, padding=1) #for kernel size=3\n",
    "#         self.conv1_2 = nn.Conv1d(2, 1, kernel_size, stride=1, padding=1) #for kernel size=3\n",
    "        \n",
    "        self.hidden3 = self.init_hidden3()\n",
    "        self.hidden2_1 = self.init_hidden2_1()\n",
    "        self.hidden2_2 = self.init_hidden2_2()\n",
    "        self.hidden2_3 = self.init_hidden2_3()\n",
    "        \n",
    "        self.hidden2label = nn.Linear(hidden_dim, label_size)\n",
    "    \n",
    "    def init_hidden3(self):\n",
    "        # the first is the hidden h\n",
    "        # the second is the cell  c\n",
    "        return (autograd.Variable(torch.zeros(self.num_layers, self.batch_size, self.hidden_dim).cuda()),\n",
    "                autograd.Variable(torch.zeros(self.num_layers, self.batch_size, self.hidden_dim).cuda()))\n",
    "    def init_hidden2_1(self):\n",
    "        # the first is the hidden h\n",
    "        # the second is the cell  c\n",
    "        return (autograd.Variable(torch.zeros(self.num_layers, self.batch_size, self.hidden_dim).cuda()),\n",
    "                autograd.Variable(torch.zeros(self.num_layers, self.batch_size, self.hidden_dim).cuda()))\n",
    "    def init_hidden2_2(self):\n",
    "        # the first is the hidden h\n",
    "        # the second is the cell  c\n",
    "        return (autograd.Variable(torch.zeros(self.num_layers, self.batch_size, self.hidden_dim).cuda()),\n",
    "                autograd.Variable(torch.zeros(self.num_layers, self.batch_size, self.hidden_dim).cuda()))\n",
    "    def init_hidden2_3(self):\n",
    "        # the first is the hidden h\n",
    "        # the second is the cell  c\n",
    "        return (autograd.Variable(torch.zeros(self.num_layers, self.batch_size, self.hidden_dim).cuda()),\n",
    "                autograd.Variable(torch.zeros(self.num_layers, self.batch_size, self.hidden_dim).cuda()))\n",
    "    \n",
    "    \n",
    "    def forward(self, joints3d_vec):\n",
    "        x3 = joints3d_vec\n",
    "        x2 = x3.view(-1, 16, 3)\n",
    "        x2_1 = x2[:,:,1:3].contiguous().view(-1, 1, 32)\n",
    "        x2_2 = x2[:,:,0:2].contiguous().view(-1, 1, 32)\n",
    "        x2_3 = x2[:,:,[0,2]].contiguous().view(-1, 1, 32)\n",
    "        \n",
    "        lstm_out3, self.hidden3 = self.lstm3(x3, self.hidden3)\n",
    "        lstm_out2_1, self.hidden2_1 = self.lstm2_1(x2_1, self.hidden2_1)\n",
    "        lstm_out2_2, self.hidden2_2 = self.lstm2_2(x2_2, self.hidden2_2)\n",
    "        lstm_out2_3, self.hidden2_3 = self.lstm2_3(x2_3, self.hidden2_3)\n",
    "        \n",
    "        t3 = lstm_out3[-1].view(self.batch_size,1,-1)\n",
    "        t2_1 = lstm_out2_1[-1].view(self.batch_size,1,-1)\n",
    "        t2_2 = lstm_out2_2[-1].view(self.batch_size,1,-1)\n",
    "        t2_3 = lstm_out2_3[-1].view(self.batch_size,1,-1)\n",
    "\n",
    "        y3 = self.conv1_3(t3)\n",
    "        y2_1 = self.conv1_2_1(t2_1)\n",
    "        y2_2 = self.conv1_2_2(t2_2)\n",
    "        y2_3 = self.conv1_2_3(t2_3)\n",
    "        \n",
    "        y3 += y2_1+y2_2+y2_3\n",
    "        \n",
    "        y3 = y3.contiguous().view(-1, self.hidden_dim)        \n",
    "        y  = self.hidden2label(y3)\n",
    "        \n",
    "        log_probs = F.softmax(y, dim=1)\n",
    "        return log_probs\n",
    "#instanstiating a model\n",
    "model0 = LSTMClassifier(48, 128, 8, 1, 2, 3)\n",
    "#to do stuff in CUDA\n",
    "model0 = model0.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "model0.load_state_dict(torch.load('../../tr_models/classifierX3_best.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ -3.80250983e-02,   4.30356972e-01,   1.40913907e-02, ...,\n",
       "          1.26750328e-02,   1.26948375e-01,  -3.62744387e-02],\n",
       "       [ -1.85335036e-01,   2.15934623e+00,   1.44080815e-01, ...,\n",
       "         -1.23556691e-01,   6.15852880e-01,  -1.86213126e-01],\n",
       "       [ -1.20000000e+01,   1.39812500e+02,   6.78567505e+00, ...,\n",
       "         -8.00000000e+00,   4.38750000e+01,  -1.03040466e+01],\n",
       "       ..., \n",
       "       [ -1.60000000e+01,   1.39750000e+02,   7.62510681e+00, ...,\n",
       "         -4.00000000e+00,   3.19375000e+01,  -1.28982391e+01],\n",
       "       [ -1.60000000e+01,   1.35750000e+02,   6.28524780e+00, ...,\n",
       "          4.00000000e+00,   4.00625000e+01,  -9.56851196e+00],\n",
       "       [ -1.60000000e+01,   1.39750000e+02,   2.14558411e+00, ...,\n",
       "          1.20000000e+01,   4.01875000e+01,  -7.95022583e+00]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v_d['val132']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = autograd.Variable(torch.from_numpy(v_d['val132']).float()).cuda()\n",
    "X1 = X.view(len(X), 1, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       "(0 ,.,.) = \n",
       "   -0.0380    0.4304    0.0141  ...     0.0127    0.1269   -0.0363\n",
       "\n",
       "(1 ,.,.) = \n",
       "   -0.1853    2.1593    0.1441  ...    -0.1236    0.6159   -0.1862\n",
       "\n",
       "(2 ,.,.) = \n",
       "  -12.0000  139.8125    6.7857  ...    -8.0000   43.8750  -10.3040\n",
       "...\n",
       "\n",
       "(49,.,.) = \n",
       "  -16.0000  139.7500    7.6251  ...    -4.0000   31.9375  -12.8982\n",
       "\n",
       "(50,.,.) = \n",
       "  -16.0000  135.7500    6.2852  ...     4.0000   40.0625   -9.5685\n",
       "\n",
       "(51,.,.) = \n",
       "  -16.0000  139.7500    2.1456  ...    12.0000   40.1875   -7.9502\n",
       "[torch.cuda.FloatTensor of size 52x1x48 (GPU 0)]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prediction is class : 2\n"
     ]
    }
   ],
   "source": [
    "y_hat = model0(X1)\n",
    "y_pred = np.argmax(y_hat.data.cpu().numpy())\n",
    "print('prediction is class : {}'.format(y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
