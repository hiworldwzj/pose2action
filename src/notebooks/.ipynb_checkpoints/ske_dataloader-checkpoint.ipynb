{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_labels = pd.read_csv('../../data/small_ntu_frames/train_labels', header=None)\n",
    "v_labels = pd.read_csv('../../data/small_ntu_frames/val_labels', header=None)\n",
    "\n",
    "with open('../../data/mini_ntu_frames/mini_train_data.pkl', 'rb') as f:\n",
    "    tr_dict = pickle.load(f)\n",
    "with open('../../data/mini_ntu_frames/mini_val_data.pkl', 'rb') as f:\n",
    "    v_dict = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_frames(arr_3d):\n",
    "    n_frames = arr_3d.shape[0]\n",
    "    temp_arr = np.zeros((n_frames, 48))\n",
    "    for i in range(n_frames):\n",
    "        arr_3d[i, :, :] = arr_3d[i, :, :] - arr_3d[i, 2, :]\n",
    "        temp_arr[i, :] = np.reshape(arr_3d[i, :, :], (1, -1))\n",
    "    return temp_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_dict(d_dict):\n",
    "    for i in d_dict:\n",
    "        d_dict[i] = flatten_frames(d_dict[i])\n",
    "    return d_dict\n",
    "\n",
    "t1 = flatten_dict(tr_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_gen(d_dict, d_labels):\n",
    "    while(True):\n",
    "        d_id = random.sample(list(d_dict), 1)[0]\n",
    "        databatch = d_dict[d_id]\n",
    "        if 'train' in d_id:\n",
    "            idx = int(d_id.replace('train', ''))\n",
    "            label = d_labels.iloc[idx, 1]\n",
    "        elif 'val' in d_id:\n",
    "            idx = int(d_id.replace('val', ''))\n",
    "            label = d_labels.iloc[idx, 1]\n",
    "        yield databatch,label\n",
    "\n",
    "Dtr = data_gen(t1, tr_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[  4.        ,  92.0625    , -10.51473999, ...,  16.        ,\n",
       "           8.25      , -28.10903931],\n",
       "        [  0.        ,  84.        ,  -5.82763672, ..., -24.        ,\n",
       "         -16.375     , -13.75427246],\n",
       "        [ 12.        ,  84.1875    ,  -6.41049194, ...,  32.        ,\n",
       "          12.5       , -17.23303223],\n",
       "        ..., \n",
       "        [ -4.        ,  87.9375    ,  -8.84054565, ..., -52.        ,\n",
       "         -16.8125    , -26.60339355],\n",
       "        [-20.        ,  91.6875    ,  -3.63687134, ...,  48.        ,\n",
       "         -59.25      , -23.83166504],\n",
       "        [ 12.        ,  84.1875    ,  -2.90161133, ...,  32.        ,\n",
       "          12.5       , -17.22859192]]), 6)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(Dtr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.autograd as autograd\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import random, numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#action LSTM\n",
    "class LSTMClassifier(nn.Module):\n",
    "    def __init__(self, joints_dim, hidden_dim, label_size, batch_size, num_layers, kernel_size):\n",
    "        super(LSTMClassifier, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.batch_size = batch_size\n",
    "        self.num_layers = num_layers\n",
    "        joints_dim2d = joints_dim - 16\n",
    "        \n",
    "        self.lstm3 = nn.LSTM(joints_dim, hidden_dim, num_layers=self.num_layers)\n",
    "        self.conv1_3 = nn.Conv1d(1, 1, kernel_size, stride=1, padding=1)\n",
    "        \n",
    "        self.lstm2_1 = nn.LSTM(joints_dim2d, hidden_dim, num_layers=self.num_layers)\n",
    "        self.conv1_2_1 = nn.Conv1d(1, 1, kernel_size, stride=1, padding=1)\n",
    "        self.lstm2_2 = nn.LSTM(joints_dim2d, hidden_dim, num_layers=self.num_layers)\n",
    "        self.conv1_2_2 = nn.Conv1d(1, 1, kernel_size, stride=1, padding=1)\n",
    "        self.lstm2_3 = nn.LSTM(joints_dim2d, hidden_dim, num_layers=self.num_layers)\n",
    "        self.conv1_2_3 = nn.Conv1d(1, 1, kernel_size, stride=1, padding=1)\n",
    "        \n",
    "#         self.conv1_1 = nn.Conv1d(4, 2, kernel_size, stride=1, padding=1) #for kernel size=3\n",
    "#         self.conv1_2 = nn.Conv1d(2, 1, kernel_size, stride=1, padding=1) #for kernel size=3\n",
    "        \n",
    "        self.hidden3 = self.init_hidden3()\n",
    "        self.hidden2_1 = self.init_hidden2_1()\n",
    "        self.hidden2_2 = self.init_hidden2_2()\n",
    "        self.hidden2_3 = self.init_hidden2_3()\n",
    "        \n",
    "        self.hidden2label = nn.Linear(hidden_dim, label_size)\n",
    "    \n",
    "    def init_hidden3(self):\n",
    "        # the first is the hidden h\n",
    "        # the second is the cell  c\n",
    "        return (autograd.Variable(torch.zeros(self.num_layers, self.batch_size, self.hidden_dim).cuda()),\n",
    "                autograd.Variable(torch.zeros(self.num_layers, self.batch_size, self.hidden_dim).cuda()))\n",
    "    def init_hidden2_1(self):\n",
    "        # the first is the hidden h\n",
    "        # the second is the cell  c\n",
    "        return (autograd.Variable(torch.zeros(self.num_layers, self.batch_size, self.hidden_dim).cuda()),\n",
    "                autograd.Variable(torch.zeros(self.num_layers, self.batch_size, self.hidden_dim).cuda()))\n",
    "    def init_hidden2_2(self):\n",
    "        # the first is the hidden h\n",
    "        # the second is the cell  c\n",
    "        return (autograd.Variable(torch.zeros(self.num_layers, self.batch_size, self.hidden_dim).cuda()),\n",
    "                autograd.Variable(torch.zeros(self.num_layers, self.batch_size, self.hidden_dim).cuda()))\n",
    "    def init_hidden2_3(self):\n",
    "        # the first is the hidden h\n",
    "        # the second is the cell  c\n",
    "        return (autograd.Variable(torch.zeros(self.num_layers, self.batch_size, self.hidden_dim).cuda()),\n",
    "                autograd.Variable(torch.zeros(self.num_layers, self.batch_size, self.hidden_dim).cuda()))\n",
    "    \n",
    "    \n",
    "    def forward(self, joints3d_vec):\n",
    "        x3 = joints3d_vec\n",
    "        x2 = x3.view(-1, 16, 3)\n",
    "        x2_1 = x2[:,:,1:3].contiguous().view(-1, 1, 32)\n",
    "        x2_2 = x2[:,:,0:2].contiguous().view(-1, 1, 32)\n",
    "        x2_3 = x2[:,:,[0,2]].contiguous().view(-1, 1, 32)\n",
    "#         print('x2_3 : ',x2_3.size())\n",
    "        lstm_out3, self.hidden3 = self.lstm3(x3, self.hidden3)\n",
    "        lstm_out2_1, self.hidden2_1 = self.lstm2_1(x2_1, self.hidden2_1)\n",
    "        lstm_out2_2, self.hidden2_2 = self.lstm2_2(x2_2, self.hidden2_2)\n",
    "        lstm_out2_3, self.hidden2_3 = self.lstm2_3(x2_3, self.hidden2_3)\n",
    "#         print('lstm_out[-1] : ', lstm_out[-1].size())\n",
    "        t3 = lstm_out3[-1].view(self.batch_size,1,-1)\n",
    "#         print('t3 : ', t3.size())\n",
    "        t2_1 = lstm_out2_1[-1].view(self.batch_size,1,-1)\n",
    "        t2_2 = lstm_out2_2[-1].view(self.batch_size,1,-1)\n",
    "        t2_3 = lstm_out2_3[-1].view(self.batch_size,1,-1)\n",
    "#         print('t2_3 : ', t2_3.size())\n",
    "        \n",
    "#         t = autograd.Variable(torch.zeros(self.batch_size, 4, self.hidden_dim).cuda())\n",
    "#         t[:,0,:] = t3\n",
    "#         t[:,1,:] = t2_1\n",
    "#         t[:,2,:] = t2_2\n",
    "#         t[:,3,:] = t2_3\n",
    "#         print('t : ', t.size())\n",
    "        \n",
    "        y3 = self.conv1_3(t3)\n",
    "#         print('y3 : ', y3.size())\n",
    "        y2_1 = self.conv1_2_1(t2_1)\n",
    "#         print('y2_1 : ', y2_1.size())\n",
    "        y2_2 = self.conv1_2_2(t2_2)\n",
    "#         print('y2_2 : ', y2_2.size())\n",
    "        y2_3 = self.conv1_2_3(t2_3)\n",
    "#         print('y2_3 : ', y2_3.size())\n",
    "        \n",
    "        y3 += y2_1+y2_2+y2_3\n",
    "        \n",
    "        y3 = y3.contiguous().view(-1, self.hidden_dim)\n",
    "#         print('y3 : ', y3.size())\n",
    "        \n",
    "        y  = self.hidden2label(y3)\n",
    "        log_probs = F.softmax(y, dim=1)\n",
    "        return log_probs\n",
    "#instanstiating a model\n",
    "model0 = LSTMClassifier(48, 512, 13, 1, 2, 3)\n",
    "#to do stuff in CUDA\n",
    "model0 = model0.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      "\n",
      "Columns 0 to 9 \n",
      " 0.4216  0.3553  0.1027  0.0555  0.5771  0.6873  0.7183  0.4206  0.0443  0.8783\n",
      "\n",
      "Columns 10 to 12 \n",
      " 0.6960  0.7852  0.9737\n",
      "[torch.FloatTensor of size 1x13]\n",
      " Variable containing:\n",
      " 1\n",
      "[torch.LongTensor of size 1]\n",
      "\n",
      "Variable containing:\n",
      " 2.7696\n",
      "[torch.FloatTensor of size 1]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "output = autograd.Variable(torch.rand(1,13))\n",
    "target = autograd.Variable(torch.LongTensor([1]))\n",
    "\n",
    "print(output, target)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "loss = criterion(output, target)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#training function\n",
    "def train(model, num_epoch, num_iter, rec_interval, disp_interval):\n",
    "    optimizer = optim.Adam(model.parameters(), lr = 1e-4)\n",
    "    loss_values = []\n",
    "    avg_loss_values = []\n",
    "    rec_step = 0\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    print('Starting the training ...')\n",
    "    for eph in range(num_epoch):\n",
    "        print('epoch {} starting ...'.format(eph))\n",
    "        avg_loss = 0\n",
    "        n_samples = 0\n",
    "        for i in range(num_iter):\n",
    "            model.hidden3 = (model.hidden3[0].detach(), model.hidden3[1].detach())\n",
    "            model.hidden2_1 = (model.hidden2_1[0].detach(), model.hidden2_1[1].detach())\n",
    "            model.hidden2_2 = (model.hidden2_2[0].detach(), model.hidden2_2[1].detach())\n",
    "            model.hidden2_3 = (model.hidden2_3[0].detach(), model.hidden2_3[1].detach())\n",
    "            model.zero_grad()\n",
    "            X,Y = next(Dtr)\n",
    "            n_samples += len(X)\n",
    "            X = autograd.Variable(torch.from_numpy(X).float()).cuda()\n",
    "            X1 = X.view(len(X), 1, -1)\n",
    "            Y = autograd.Variable(torch.LongTensor(np.array([Y]))).cuda()\n",
    "\n",
    "            y_hat = model(X1)\n",
    "#             print('before loss \\n', y_hat, Y)\n",
    "            loss = criterion(y_hat, Y)\n",
    "#             loss_n = loss.data[0]\n",
    "#             print('after loss \\n', eph, i, y_hat, loss_n)\n",
    "            avg_loss += loss.data[0]\n",
    "            \n",
    "            if i % disp_interval == 0:\n",
    "                print('epoch: %d iterations: %d loss :%g' % (eph, i, loss.data[0]))\n",
    "            if rec_step%rec_interval==0:\n",
    "                loss_values.append(loss.data[0])\n",
    "            \n",
    "            loss.backward()     \n",
    "            optimizer.step()\n",
    "            rec_step += 1\n",
    "            \n",
    "        avg_loss /= n_samples\n",
    "        avg_loss_values.append(avg_loss)\n",
    "#         #evaluating model accuracy\n",
    "# #         acc = evaluate_accuracy(model, test_split)\n",
    "        print('epoch: {} <====train track===> avg_loss: {}, accuracy \\n'.format(eph, avg_loss))\n",
    "    return loss_values, avg_loss_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting the training ...\n",
      "epoch 0 starting ...\n",
      "epoch: 0 iterations: 0 loss :1.6927\n",
      "epoch: 0 iterations: 10 loss :1.68932\n",
      "epoch: 0 iterations: 20 loss :1.68944\n",
      "epoch: 0 iterations: 30 loss :1.68924\n",
      "epoch: 0 iterations: 40 loss :1.68912\n",
      "epoch: 0 iterations: 50 loss :1.68915\n",
      "epoch: 0 iterations: 60 loss :1.68911\n",
      "epoch: 0 iterations: 70 loss :1.68911\n",
      "epoch: 0 iterations: 80 loss :1.68912\n",
      "epoch: 0 iterations: 90 loss :1.68912\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "tuple index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-5271b2ab6a83>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mloss_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mavg_loss_vals\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#100eph_8e-6,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mavg_loss_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-15-e335caa1ba1f>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, num_epoch, num_iter, rec_interval, disp_interval)\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;31m#         #evaluating model accuracy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;31m# #         acc = evaluate_accuracy(model, test_split)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'epoch: {} <====train track===> avg_loss: {}, accuracy: {}% \\n'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mavg_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mloss_values\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mavg_loss_values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: tuple index out of range"
     ]
    }
   ],
   "source": [
    "loss_vals, avg_loss_vals = train(model0, 10, 100, 2, 10) #100eph_8e-6, \n",
    "plt.figure()\n",
    "plt.plot(loss_vals)\n",
    "plt.figure()\n",
    "plt.plot(avg_loss_vals)\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('avg loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 0.1454\n",
      " 0.4330\n",
      " 0.2739\n",
      "[torch.FloatTensor of size 3]\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z = torch.rand(3)\n",
    "print(z)\n",
    "np.argmax(z.numpy())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
